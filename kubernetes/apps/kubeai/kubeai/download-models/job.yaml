apiVersion: batch/v1
kind: Job
metadata:
  name: ollama-load-models-to-nfs
spec:
  template:
    spec:
      containers:
        - name: ollama
          image: ollama/ollama:latest
          env:
            - name: OLLAMA_MODELS
              value: "gemma3:4b"
          command:
            - /bin/sh
            - -c
            - |
              /bin/ollama serve &
              echo "Waiting for Ollama server to start..."
              sleep 10

              for model in $OLLAMA_MODELS; do
                # Replace punctuation with dash for folder name
                safe_name=$(echo "$model" | sed 's/[^a-zA-Z0-9]/-/g')
                model_path="/model/$safe_name"
                echo "Pulling model $model into $model_path..."

                mkdir -p "$model_path"
                export OLLAMA_MODELS_DIR="$model_path"

                if ! /bin/ollama pull "$model"; then
                  echo "Failed to pull model: $model"
                  exit 1
                fi
              done

              echo "Listing model directories and files:"
              find /model -type f
          volumeMounts:
            - name: models-volume
              mountPath: /model
      volumes:
        - name: models-volume
          nfs:
            server: nas.internal
            path: /mnt/world/minio/kubeai
            readOnly: false
      restartPolicy: OnFailure
---
apiVersion: batch/v1
kind: Job
metadata:
  name: hf-download-models-to-nfs
spec:
  template:
    spec:
      containers:
        - name: hf-downloader
          image: python:3.10-slim
          env:
            - name: HF_MODELS
              value: "Systran/faster-whisper-small.en"
          command:
            - /bin/sh
            - -c
            - |
              echo "Installing dependencies..."
              pip install --no-cache-dir "huggingface_hub[cli]"

              for model in $HF_MODELS; do
                # Create a safe folder name by replacing punctuation with dashes
                safe_name=$(echo "$model" | sed 's/[^a-zA-Z0-9]/-/g')
                model_path="/models/$safe_name"
                echo "Downloading Hugging Face model $model into $model_path"

                mkdir -p "$model_path"

                # Use huggingface-cli via Python script to download model
                huggingface-cli download $model --local-dir $model_path
              done

              echo "Model download complete. Listing files:"
              find /models -type f
          volumeMounts:
            - name: models-volume
              mountPath: /models
      volumes:
        - name: models-volume
          nfs:
            server: nas.internal
            path: /mnt/world/minio/kubeai
            readOnly: false
      restartPolicy: OnFailure
