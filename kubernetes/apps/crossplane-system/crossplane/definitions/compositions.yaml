---
apiVersion: apiextensions.crossplane.io/v1
kind: Composition
metadata:
  name: modal-llm
  labels:
    provider: modal
spec:
  compositeTypeRef:
    apiVersion: goyangi.io/v1alpha1
    kind: XModalLLM
  mode: Pipeline
  pipeline:
    - step: render-templates
      functionRef:
        name: function-go-templating
      input:
        apiVersion: gotemplating.fn.crossplane.io/v1beta1
        kind: GoTemplate
        source: Inline
        inline:
          template: |
            ---
            apiVersion: kubernetes.crossplane.io/v1alpha2
            kind: Object
            metadata:
              name: modal-llm-configmap-{{ .observed.composite.resource.metadata.name }}
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: modal-app-configmap
            spec:
              forProvider:
                manifest:
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: modal-llm-{{ .observed.composite.resource.metadata.name }}
                    namespace: crossplane-system
                  data:
                    {{- $spec := .observed.composite.resource.spec }}
                    {{- $name := .observed.composite.resource.metadata.name }}
                    {{- if eq $spec.engine "sglang" }}
                    app.py: |
                      import os
                      import subprocess
                      import time
                      import modal
                      import requests

                      MINUTES = 60

                      sglang_image = (
                          modal.Image.from_registry("lmsysorg/sglang:v0.5.6.post2-cu129-amd64-runtime")
                          .entrypoint([])
                          .uv_pip_install("huggingface-hub==0.36.0", "requests")
                          .env({
                              "HF_HUB_CACHE": "/root/.cache/huggingface",
                              "HF_XET_HIGH_PERFORMANCE": "1",
                              "TORCHINDUCTOR_COMPILE_THREADS": "1",
                          })
                      )

                      hf_cache_vol = modal.Volume.from_name("huggingface-cache", create_if_missing=True)

                      MODEL_NAME = "{{ $spec.model }}"
                      {{- if $spec.revision }}
                      MODEL_REVISION = "{{ $spec.revision }}"
                      {{- else }}
                      MODEL_REVISION = "main"
                      {{- end }}
                      N_GPU = {{ $spec.gpuCount | default 1 }}
                      PORT = 8000
                      TARGET_INPUTS = {{ $spec.maxInputs | default 32 }}

                      app = modal.App("llm-{{ $name }}")

                      def wait_ready(process, timeout=5 * MINUTES):
                          deadline = time.time() + timeout
                          while time.time() < deadline:
                              if (rc := process.poll()) is not None:
                                  raise subprocess.CalledProcessError(rc, cmd=process.args)
                              try:
                                  requests.get(f"http://127.0.0.1:{PORT}/health").raise_for_status()
                                  return
                              except (ConnectionError, requests.exceptions.RequestException):
                                  time.sleep(1)
                          raise TimeoutError(f"SGLang server not ready within {timeout}s")

                      def warmup():
                          for _ in range(3):
                              requests.post(f"http://127.0.0.1:{PORT}/v1/chat/completions", json={
                                  "model": MODEL_NAME,
                                  "messages": [{"role": "user", "content": "Hello"}],
                                  "max_tokens": 8,
                              }).raise_for_status()

                      def sleep():
                          requests.post(f"http://127.0.0.1:{PORT}/release_memory_occupation", json={}).raise_for_status()

                      def wake_up():
                          requests.post(f"http://127.0.0.1:{PORT}/resume_memory_occupation", json={}).raise_for_status()

                      {{- if $spec.snapshot }}
                      @app.cls(
                          image=sglang_image,
                          gpu="{{ $spec.gpu | default "L4" }}:{}".format(N_GPU) if N_GPU > 1 else "{{ $spec.gpu | default "L4" }}",
                          volumes={"/root/.cache/huggingface": hf_cache_vol},
                          scaledown_window={{ $spec.scaledownMinutes | default 10 }} * MINUTES,
                          timeout={{ $spec.timeoutMinutes | default 30 }} * MINUTES,
                          secrets=[modal.Secret.from_name("llm-api-key")],
                          enable_memory_snapshot=True,
                          experimental_options={"enable_gpu_snapshot": True},
                      )
                      @modal.concurrent(max_inputs=TARGET_INPUTS)
                      class SGLang:
                          @modal.enter(snap=True)
                          def startup(self):
                              cmd = [
                                  "python", "-m", "sglang.launch_server",
                                  "--model-path", MODEL_NAME,
                                  "--revision", MODEL_REVISION,
                                  "--served-model-name", MODEL_NAME,
                                  "--host", "0.0.0.0",
                                  "--port", str(PORT),
                                  "--tp", str(N_GPU),
                                  "--cuda-graph-max-bs", str(TARGET_INPUTS * 2),
                                  "--max-running-requests", str(TARGET_INPUTS),
                                  "--context-length", "{{ $spec.maxModelLen | default 32768 }}",
                                  "--api-key", os.environ["API_KEY"],
                                  "--enable-metrics",
                                  "--enable-memory-saver",
                                  "--enable-weights-cpu-backup",
                                  {{- if $spec.mistralFormat }}
                                  "--tokenizer-mode", "mistral",
                                  "--config-format", "mistral",
                                  "--load-format", "mistral",
                                  {{- end }}
                              ]
                              self.process = subprocess.Popen(cmd)
                              wait_ready(self.process)
                              warmup()
                              sleep()

                          @modal.enter(snap=False)
                          def restore(self):
                              wake_up()

                          @modal.web_server(port=PORT, startup_timeout=10 * MINUTES)
                          def serve(self):
                              pass

                          @modal.exit()
                          def stop(self):
                              self.process.terminate()
                      {{- else }}
                      @app.function(
                          image=sglang_image,
                          gpu="{{ $spec.gpu | default "L4" }}:{}".format(N_GPU) if N_GPU > 1 else "{{ $spec.gpu | default "L4" }}",
                          volumes={"/root/.cache/huggingface": hf_cache_vol},
                          scaledown_window={{ $spec.scaledownMinutes | default 10 }} * MINUTES,
                          timeout={{ $spec.timeoutMinutes | default 30 }} * MINUTES,
                          secrets=[modal.Secret.from_name("llm-api-key")],
                      )
                      @modal.concurrent(max_inputs=TARGET_INPUTS)
                      @modal.web_server(port=PORT, startup_timeout=30 * MINUTES)
                      def serve():
                          cmd = [
                              "python", "-m", "sglang.launch_server",
                              "--model-path", MODEL_NAME,
                              "--revision", MODEL_REVISION,
                              "--served-model-name", MODEL_NAME,
                              "--host", "0.0.0.0",
                              "--port", str(PORT),
                              "--tp", str(N_GPU),
                              "--cuda-graph-max-bs", str(TARGET_INPUTS * 2),
                              "--context-length", "{{ $spec.maxModelLen | default 32768 }}",
                              "--api-key", os.environ["API_KEY"],
                              {{- if $spec.mistralFormat }}
                              "--tokenizer-mode", "mistral",
                              "--config-format", "mistral",
                              "--load-format", "mistral",
                              {{- end }}
                          ]
                          subprocess.Popen(cmd)
                      {{- end }}
                    {{- else }}
                    app.py: |
                      import json
                      import os
                      import subprocess
                      import modal

                      MINUTES = 60

                      vllm_image = (
                          modal.Image.from_registry("nvidia/cuda:12.8.1-devel-ubuntu22.04", add_python="3.12")
                          .entrypoint([])
                          .uv_pip_install("vllm==0.8.5", "huggingface_hub[hf_transfer]==0.36.0")
                          .env({"VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8": "1"})
                      )

                      hf_cache_vol = modal.Volume.from_name("huggingface-cache", create_if_missing=True)
                      vllm_cache_vol = modal.Volume.from_name("vllm-cache", create_if_missing=True)

                      MODEL_NAME = "{{ $spec.model }}"
                      {{- if $spec.revision }}
                      MODEL_REVISION = "{{ $spec.revision }}"
                      {{- else }}
                      MODEL_REVISION = "main"
                      {{- end }}
                      N_GPU = {{ $spec.gpuCount | default 1 }}
                      PORT = 8000
                      MAX_INPUTS = {{ $spec.maxInputs | default 32 }}

                      app = modal.App("llm-{{ $name }}")

                      @app.function(
                          image=vllm_image,
                          gpu="{{ $spec.gpu | default "L4" }}:{}".format(N_GPU) if N_GPU > 1 else "{{ $spec.gpu | default "L4" }}",
                          scaledown_window={{ $spec.scaledownMinutes | default 10 }} * MINUTES,
                          timeout={{ $spec.timeoutMinutes | default 30 }} * MINUTES,
                          volumes={
                              "/root/.cache/huggingface": hf_cache_vol,
                              "/root/.cache/vllm": vllm_cache_vol,
                          },
                          secrets=[modal.Secret.from_name("llm-api-key")],
                      )
                      @modal.concurrent(max_inputs=MAX_INPUTS)
                      @modal.web_server(port=PORT, startup_timeout=30 * MINUTES)
                      def serve():
                          cmd = [
                              "vllm", "serve", MODEL_NAME,
                              "--revision", MODEL_REVISION,
                              "--served-model-name", MODEL_NAME, "llm",
                              "--host", "0.0.0.0",
                              "--port", str(PORT),
                              "--tensor-parallel-size", str(N_GPU),
                              "--kv-cache-dtype", "{{ $spec.kvCacheDtype | default "fp8" }}",
                              "--max-num-batched-tokens", "{{ $spec.maxBatchedTokens | default 16384 }}",
                              "--max-model-len", "{{ $spec.maxModelLen | default 32768 }}",
                              "--api-key", os.environ["API_KEY"],
                          ]
                          {{- if and $spec.speculative $spec.speculative.enabled }}
                          speculative_config = {
                              "model": "{{ $spec.speculative.model }}",
                              "num_speculative_tokens": {{ $spec.speculative.numTokens | default 7 }},
                              "method": "{{ $spec.speculative.method | default "eagle3" }}",
                          }
                          cmd += ["--speculative-config", json.dumps(speculative_config)]
                          {{- end }}
                          print(*cmd)
                          subprocess.Popen(cmd)
                    {{- end }}
            ---
            apiVersion: kubernetes.crossplane.io/v1alpha2
            kind: Object
            metadata:
              name: modal-deploy-job-{{ .observed.composite.resource.metadata.name }}
              annotations:
                gotemplating.fn.crossplane.io/composition-resource-name: modal-deploy-job
            spec:
              forProvider:
                manifest:
                  apiVersion: batch/v1
                  kind: Job
                  metadata:
                    name: modal-deploy-{{ .observed.composite.resource.metadata.name }}
                    namespace: crossplane-system
                  spec:
                    ttlSecondsAfterFinished: 300
                    template:
                      spec:
                        restartPolicy: OnFailure
                        containers:
                          - name: modal-deploy
                            image: python:3.12-slim
                            command:
                              - /bin/sh
                              - -c
                              - |
                                pip install modal -q
                                modal deploy /app/app.py
                            envFrom:
                              - secretRef:
                                  name: modal-credentials
                            volumeMounts:
                              - name: app
                                mountPath: /app
                        volumes:
                          - name: app
                            configMap:
                              name: modal-llm-{{ .observed.composite.resource.metadata.name }}
    - step: auto-ready
      functionRef:
        name: function-auto-ready
  writeConnectionSecretsToNamespace: crossplane-system
