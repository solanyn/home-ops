apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama3
spec:
  predictor:
    model:
      modelFormat:
        name: huggingface
      args:
        - --model_name=llama3
        - --model_id=meta-llama/Llama-3.2-1B-Instruct
        - --kv-transfer-config
        - '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_both"}'
        - --enable-chunked-prefill
      env:
        - name: LMCACHE_USE_EXPERIMENTAL
          value: "True"
        - name: LMCACHE_REMOTE_URL
          value: redis://redis.storage.svc.cluster.local:6379
        - name: LMCACHE_REMOTE_SERDE
          value: naive
        - name: LMCACHE_LOCAL_CPU
          value: "True"
        - name: LMCACHE_CHUNK_SIZE
          value: "256"
        - name: LMCACHE_MAX_LOCAL_CPU_SIZE
          value: "2.0"
      envFrom:
        - secretRef:
            name: kserve-models-secret
      resources:
        requests:
          cpu: 10m
          requests: 64Mi
