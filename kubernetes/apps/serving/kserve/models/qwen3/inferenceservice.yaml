apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: qwen3
spec:
  predictor:
    model:
      image: docker.io/kserve/huggingfaceserver:latest@sha256:16eb022561e8f0eef57dbbe1850452af95efb5e1dfd8a49850afb36e4ddec7f0
      modelFormat:
        name: huggingface
      args:
        - --model_name=qwen3
        - --model_id=Qwen/Qwen3-1.7B
        - --backend=huggingface
        - --kv-transfer-config
        - '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_both"}'
        - --enable-chunked-prefill
      env:
        - name: KSERVE_OPENAI_ROUTE_PREFIX
          value: ""
        - name: LMCACHE_USE_EXPERIMENTAL
          value: "True"
        - name: LMCACHE_REMOTE_URL
          value: redis://redis.storage.svc.cluster.local:6379
        - name: LMCACHE_REMOTE_SERDE
          value: naive
        - name: LMCACHE_LOCAL_CPU
          value: "True"
        - name: LMCACHE_CHUNK_SIZE
          value: "256"
        - name: LMCACHE_MAX_LOCAL_CPU_SIZE
          value: "2.0"
      envFrom:
        - secretRef:
            name: kserve-models-secret
      resources:
        requests:
          cpu: 10m
          memory: 64Mi
