include:
  - model_config.yaml
general_settings:
  proxy_batch_write_at: 60
  database_connection_pool_limit: 20
  disable_spend_logs: false
  disable_error_logs: false
  background_health_checks: false
  health_check_interval: 300
  store_model_in_db: true
  redis_host: os.environ/REDIS_HOST
  redis_port: os.environ/REDIS_PORT
  alerting: ["email"]
litellm_settings:
  proxy_server: true
  request_timeout: 600
  json_logs: true
  # enable_preview_features: true
  redact_user_api_key_info: true
  turn_off_message_logging: false
  set_verbose: true
  drop_params: true
  # Caching settings
  cache: true
  cache_params:
    type: redis
    host: os.environ/REDIS_HOST
    port: os.environ/REDIS_PORT
    redis_semantic_cache_embedding_model: text-embedding-3-small # the model should be defined on the model_list
    redis_semantic_cache_index_name: litellm_cache
    similarity_threshold: 0.98 # similarity threshold for semantic cache
    # /chat/completions, /completions, /embeddings, /audio/transcriptions
    supported_call_types:
      ["acompletion", "atext_completion", "aembedding", "atranscription"]
    mode: default_on
    ttl: 600 # ttl for caching
router:
  - destination: vision_openai
    if:
      contains:
        - "data:image"
        - "image analysis"
        - "describe this image"
        - "photo"
        - "diagram"
router_settings:
  fallbacks:
    - vision_openai:
        - vision_anthropic_4_opus
        - vision_anthropic_4_sonnet
        - vision_anthropic_3.7_sonnet
        - vision_gemini
    - general_openai:
        - general_anthropic_4_opus
        - vision_anthropic_4_sonnet
        - vision_anthropic_3.7_sonnet
        - general_gemini
    - code_fallback_together:
        - code_fallback_groq
    - code_fallback_groq:
        - code_fallback_together
