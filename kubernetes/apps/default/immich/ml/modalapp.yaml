---
apiVersion: modal.internal.io/v1alpha1
kind: ModalApp
metadata:
  name: immich-ml
spec:
  appName: immich-ml
  servicePort: 80
  source: |
    import modal
    import sys

    MINUTES = 60
    PORT = 3003

    immich_ml_image = (
        modal.Image.from_registry(
            "ghcr.io/immich-app/immich-machine-learning:v2.5.6-cuda",
            add_python=None,
        )
    )

    model_cache_vol = modal.Volume.from_name("immich-ml-cache", create_if_missing=True)

    app = modal.App("immich-ml")

    @app.function(
        image=immich_ml_image,
        gpu="T4",
        scaledown_window=2 * MINUTES,
        timeout=30 * MINUTES,
        volumes={
            "/cache": model_cache_vol,
        },
        memory=4096,
    )
    @modal.concurrent(max_inputs=4)
    @modal.web_server(port=PORT, startup_timeout=10 * MINUTES)
    def serve():
        import subprocess
        import os
        env = os.environ.copy()
        env["MACHINE_LEARNING_CACHE_FOLDER"] = "/cache"
        env["TRANSFORMERS_CACHE"] = "/cache"
        env["PYTHONPATH"] = "/usr/src/app"
        subprocess.Popen(
            ["gunicorn", "app.main:app", "-k", "uvicorn.workers.UvicornWorker",
             "-b", f"0.0.0.0:{PORT}", "-w", "1", "-t", "0"],
            env=env,
        )
