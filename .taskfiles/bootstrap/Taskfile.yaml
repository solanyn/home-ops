---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: '3'

tasks:

  talos:
    desc: Bootstrap Talos
    cmds:
      - for: { var: TALOS_NODES }
        cmd: >
          sops exec-file --input-type yaml --output-type yaml {{.ITEM}} "minijinja-cli {}"
          | talosctl --nodes {{base .ITEM | replace ".sops.yaml.j2" ""}} apply-config --insecure --file /dev/stdin
      - until talosctl --nodes {{.TALOS_CONTROLLER}} bootstrap; do sleep 5; done
      - talosctl kubeconfig --nodes {{.TALOS_CONTROLLER}} --force --force-context-name main {{.KUBERNETES_DIR}}
    vars:
      TALOS_CONTROLLER:
        sh: talosctl config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
      TALOS_NODES:
        sh: ls {{.KUBERNETES_DIR}}/bootstrap/talos/*.j2
    preconditions:
      - talosctl config info
      - test -f {{.KUBERNETES_DIR}}/talosconfig
      - which jq minijinja-cli sops talosctl

  apps:
    desc: Bootstrap Apps [ROOK_DISK=required]
    summary: |
      IMPORTANT: All nodes will be used for OSDs and ROOK_DISK must be set to a value that matches the disk model on all nodes (e.g. Micron_7450)
    prompt: Bootstrap apps into the Talos cluster?
    cmds:
      # - until kubectl wait nodes --for=condition=Ready=False --all --timeout=10m; do sleep 5; done
      - op run --env-file {{.KUBERNETES_DIR}}/bootstrap/apps/.secrets.env --no-masking -- minijinja-cli {{.KUBERNETES_DIR}}/bootstrap/apps/templates/resources.yaml.j2 | kubectl apply --server-side --filename -
      - helmfile --quiet --file {{.KUBERNETES_DIR}}/bootstrap/apps/helmfile.yaml apply --skip-diff-on-install --suppress-diff
      - helmfile --quiet --file {{.KUBERNETES_DIR}}/bootstrap/apps/helmfile.yaml destroy --selector name=wipe-rook
    env:
      NODE_COUNT:
        sh: talosctl config info --output json | jq --raw-output '.nodes | length'
      ROOK_DISK: '{{.ROOK_DISK}}'
    requires:
      vars: [ROOK_DISK]
    preconditions:
      - op user get --me
      - talosctl config info
      - test -f {{.KUBERNETES_DIR}}/talosconfig
      - test -f {{.KUBERNETES_DIR}}/bootstrap/apps/helmfile.yaml
      - test -f {{.KUBERNETES_DIR}}/bootstrap/apps/templates/resources.yaml.j2
      - test -f {{.KUBERNETES_DIR}}/bootstrap/apps/templates/wipe-rook.yaml.gotmpl
      - which curl jq helmfile kubectl op talosctl

  cnpg:
    desc: Increments cloudnative-pg cluster version
    prompt: Increment cloudnative-pg cluster version, commit and push?
    cmds:
      - yq eval ".spec.backup.barmanObjectStore.serverName = \"{{.CLUSTER_NAME}}-v{{.CURRENT | add1 }}\"" -i {{.CLUSTER_DIR}}/apps/database/cloudnative-pg/cluster/cluster16.yaml
      - yq eval ".spec.bootstrap.recovery.source = \"{{.CLUSTER_NAME}}-v{{.PREVIOUS | add1 }}\"" -i {{.CLUSTER_DIR}}/apps/database/cloudnative-pg/cluster/cluster16.yaml
      - git add {{.CLUSTER_DIR}}/apps/database/cloudnative-pg/cluster/cluster16.yaml
      - "git commit -m 'chore(cnpg): increment cluster task'"
      - git push
    vars:
      CLUSTER_NAME:
        sh: yq eval '.metadata.name' {{.CLUSTER_DIR}}/apps/database/cloudnative-pg/cluster/cluster16.yaml
      CURRENT:
        sh: yq eval '.spec.backup.barmanObjectStore.serverName | split("-v") | .[1] | tonumber' {{.CLUSTER_DIR}}/apps/database/cloudnative-pg/cluster/cluster16.yaml
      PREVIOUS:
        sh: yq eval '.spec.bootstrap.recovery.source | split("-v") | .[1] | tonumber' {{.CLUSTER_DIR}}/apps/database/cloudnative-pg/cluster/cluster16.yaml
    preconditions:
      - test -f {{.CLUSTER_DIR}}/apps/database/cloudnative-pg/cluster/cluster16.yaml
      - which yq git
